{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Function Calling\n",
    "One of the many ways to provide an LLM with real-time, dynamic data is to let it call functions. Or, to be more specifc, let it tell you to call a function for it and then provide the results so it can use it to fulfill a query.\n",
    "\n",
    "In this notebook, I will show you how to implement function calling using the OpenAI API and GPT-4o-mini. We will create an analytics agent that can query data from a database to answer analytical questions we can ask in natural language!\n",
    "\n",
    "I will use DuckDB as a light-weight in-memory database. This would work with any other database, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Some Data\n",
    "I found some product price index data set on Kaggle, which shows the historic price evolution of some foods in different US cities over the course of two decades. Let's load it to a table called product_prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "    \"\"\"\n",
    "    CREATE TABLE product_prices AS\n",
    "    SELECT * FROM read_csv('ProductPriceIndex.csv');\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────┬────────────┬───────────┬───────────────┬───────────────┬──────────────────┬───────────────┬───────────────┐\n",
      "│   productname    │    date    │ farmprice │ atlantaretail │ chicagoretail │ losangelesretail │ newyorkretail │ averagespread │\n",
      "│     varchar      │    date    │  varchar  │    varchar    │    varchar    │     varchar      │    varchar    │    varchar    │\n",
      "├──────────────────┼────────────┼───────────┼───────────────┼───────────────┼──────────────────┼───────────────┼───────────────┤\n",
      "│ Strawberries     │ 2019-05-19 │ $1.16     │ $2.23         │ $1.70         │ $1.99            │ $2.54         │ 82.33%        │\n",
      "│ Romaine Lettuce  │ 2019-05-19 │ $0.35     │ $1.72         │ $2.00         │ $1.69            │ $1.99         │ 428.57%       │\n",
      "│ Red Leaf Lettuce │ 2019-05-19 │ $0.32     │ $1.84         │ $1.84         │ $1.69            │ $1.89         │ 467.19%       │\n",
      "│ Potatoes         │ 2019-05-19 │ $1.50     │ $5.32         │ $5.14         │ $3.99            │ $6.22         │ 244.50%       │\n",
      "│ Oranges          │ 2019-05-19 │ $0.41     │ $1.42         │ $1.45         │ $1.34            │ $2.05         │ 281.71%       │\n",
      "└──────────────────┴────────────┴───────────┴───────────────┴───────────────┴──────────────────┴───────────────┴───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM product_prices LIMIT 5;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬─────────────┐\n",
      "│  min_date  │  max_date  │ num_records │\n",
      "│    date    │    date    │    int64    │\n",
      "├────────────┼────────────┼─────────────┤\n",
      "│ 1999-10-24 │ 2019-05-19 │       15766 │\n",
      "└────────────┴────────────┴─────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT min(date) AS min_date, max(date) AS max_date, COUNT(*) as num_records FROM product_prices;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┐\n",
      "│   dates    │\n",
      "│    date    │\n",
      "├────────────┤\n",
      "│ 2019-05-19 │\n",
      "│ 2019-05-12 │\n",
      "│ 2019-05-05 │\n",
      "│ 2019-04-28 │\n",
      "│ 2019-04-21 │\n",
      "│ 2019-04-14 │\n",
      "│ 2019-04-07 │\n",
      "│ 2019-03-31 │\n",
      "│ 2019-03-24 │\n",
      "│ 2019-03-17 │\n",
      "├────────────┤\n",
      "│  10 rows   │\n",
      "└────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT distinct(date) as dates FROM product_prices ORDER BY dates DESC LIMIT 10;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a column called productname, a date column and a bunch of columns with price information for different cities. The information ranges from 1999 to 2019 and seems to give us an overview of price evolution for different products on a weekly basis. \n",
    "\n",
    "Let's assume this is a production database that changes over time. Products and new price information can be added. The LLM will need to be able to access this information on-demand. We will give it the capabilities to call functions that run queries and return the results for it to use to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Query Functions\n",
    "\n",
    "First, let's create a decorator function called *result_to_json* to get the DuckDB query results in JSON format so we can pass them to the LLM as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from duckdb.duckdb import DuckDBPyRelation\n",
    "\n",
    "def result_to_json(func: Callable[..., DuckDBPyRelation]) -> Callable[..., str]:\n",
    "    def inner(*args, **kwargs) -> str:\n",
    "        result = func(*args, **kwargs)\n",
    "        return result.fetchdf().to_json(orient=\"records\", date_format=\"iso\")\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I will create functions to do the following things:\n",
    "- Get sample data\n",
    "- Get all distinct products in the database\n",
    "- Current the most recent prices for a specific product\n",
    "- All records for a product in a date range\n",
    "\n",
    "Of course, in a real-world scenario we would have to pinpoint these capabilities to match business needs and frequent questions. For now, let's assume these are important informations that are requested frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "@result_to_json\n",
    "def get_data_sample() -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        \"\"\"\n",
    "        SELECT * FROM product_prices LIMIT 5;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_distinct_products() -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        \"\"\"\n",
    "        SELECT distinct(productname) FROM product_prices;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_current_prices(product_name: str) -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT * \n",
    "        FROM product_prices\n",
    "        WHERE productname = \\'{product_name}\\'\n",
    "        AND date = (\n",
    "            SELECT MAX(date)\n",
    "            FROM product_prices\n",
    "            WHERE productname = '{product_name}'\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_records_in_date_range(product_name: str, min_date: str, max_date: str):\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM product_prices\n",
    "        WHERE productname = \\'{product_name}\\'\n",
    "        AND date >= \\'{min_date}\\'\n",
    "        AND date <= \\'{max_date}\\'\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chatbot Using the OpenAI API\n",
    "So far, so good. Now, let's create a simple chatbot that we can interact with which uses the OpenAI API for LLM integration. We will later integrate the above function calls with the LLM, making it a little data analysis agent.\n",
    "\n",
    "We start by crafting our system prompt to tell the LLM its purpose. In the system prompt, I will use the outputs  of the *get_sample_data* and *get_distinct_products* functions so the LLM knows about the data format and which products it can answer questions for. We will also tell the LLM in the system prompt that today is the max. date of our dataset, 2019–05–19, so we can treat the data as if it is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are a helpful assistant that analyzes data. You will answer questions about the price development of different food products based\n",
    "on data stored in a database. \n",
    "\n",
    "Here is a sample of what the data that you will analyze looks like:\n",
    "{get_data_sample()}\n",
    "\n",
    "The database contains information about the following distinct products: \n",
    "{get_distinct_products()}\n",
    "\n",
    "Today's date is 2019-05-19. Data is available from 1999-10-24 til 2019-05-19. \n",
    "\n",
    "Only answer questions related to analyzing the product price data. If the user requests you to do anything else, \n",
    "kindly tell them that you are here to analyze product price data and that you cannot help with other requests.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant that analyzes data. You will answer questions about the price development of different food products based\n",
      "on data stored in a database. \n",
      "\n",
      "Here is a sample of what the data that you will analyze looks like:\n",
      "[{\"productname\":\"Strawberries\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$1.16\",\"atlantaretail\":\"$2.23\",\"chicagoretail\":\"$1.70\",\"losangelesretail\":\"$1.99\",\"newyorkretail\":\"$2.54\",\"averagespread\":\"82.33%\"},{\"productname\":\"Romaine Lettuce\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.35\",\"atlantaretail\":\"$1.72\",\"chicagoretail\":\"$2.00\",\"losangelesretail\":\"$1.69\",\"newyorkretail\":\"$1.99\",\"averagespread\":\"428.57%\"},{\"productname\":\"Red Leaf Lettuce\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.32\",\"atlantaretail\":\"$1.84\",\"chicagoretail\":\"$1.84\",\"losangelesretail\":\"$1.69\",\"newyorkretail\":\"$1.89\",\"averagespread\":\"467.19%\"},{\"productname\":\"Potatoes\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$1.50\",\"atlantaretail\":\"$5.32\",\"chicagoretail\":\"$5.14\",\"losangelesretail\":\"$3.99\",\"newyorkretail\":\"$6.22\",\"averagespread\":\"244.50%\"},{\"productname\":\"Oranges\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.41\",\"atlantaretail\":\"$1.42\",\"chicagoretail\":\"$1.45\",\"losangelesretail\":\"$1.34\",\"newyorkretail\":\"$2.05\",\"averagespread\":\"281.71%\"}]\n",
      "\n",
      "The database contains information about the following distinct products: \n",
      "[{\"productname\":\"Thompson Grapes\"},{\"productname\":\"Celery\"},{\"productname\":\"Peaches\"},{\"productname\":\"Potatoes\"},{\"productname\":\"Oranges\"},{\"productname\":\"Green Leaf Lettuce\"},{\"productname\":\"Carrots\"},{\"productname\":\"Cantaloupe\"},{\"productname\":\"Avocados\"},{\"productname\":\"Strawberries\"},{\"productname\":\"Romaine Lettuce\"},{\"productname\":\"Broccoli Crowns\"},{\"productname\":\"Asparagus\"},{\"productname\":\"Tomatoes\"},{\"productname\":\"Cauliflower\"},{\"productname\":\"Plums\"},{\"productname\":\"Iceberg Lettuce\"},{\"productname\":\"Honeydews\"},{\"productname\":\"Red Leaf Lettuce\"},{\"productname\":\"Broccoli Bunches\"},{\"productname\":\"Flame Grapes\"},{\"productname\":\"Nectarines\"}]\n",
      "\n",
      "Today's date is 2019-05-19. Data is available from 1999-10-24 til 2019-05-19. \n",
      "\n",
      "Only answer questions related to analyzing the product price data. If the user requests you to do anything else, \n",
      "kindly tell them that you are here to analyze product price data and that you cannot help with other requests.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the user prompts dynamic, we will leverage a Gradio chat UI that dynamically passes the user input, the previous conversation, as well as the system prompt to the OpenAI model.\n",
    "\n",
    "The Gradio ChatInterface object expects us to pass a callback method that takes exactly two arguments: *message* and *history*. The first one is the new user input message, while the history is the whole previous conversation between the user and the LLM. For more information, you can check the Gradio docs here: https://www.gradio.app/docs/gradio/chatinterface\n",
    "\n",
    "For now, let's create the callback method for the chat interface and launch the Gradio UI. In the callback method, we will use the latest user message, the message history and the system prompt to build our *messages* API input for the GPT-4o-mini model. Like always, we craft our model input passing a list of JSON objects that looks as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": <SYSTEM_PROMPT>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE>}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the model to see the full previous conversation, we will extend that list with additional objects. For each LLM response, we will add an object that uses the role *assistant*. This feature can be used to pass the full previous conversation, but also to pass responses from other agents that assist fulfilling a user query. For every new user message, we will append an object that uses the role *user*.\n",
    "\n",
    "Over time, the messages input for the OpenAI API will look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": <SYSTEM_PROMPT>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE_1>},\n",
    "    {\"role\": \"assistant\", \"content\": <GPT_RESPONSE_1>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE_2>},\n",
    "    {\"role\": \"assistant\", \"content\": <GPT_RESPONSE_2>},\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing the full conversation to the API every time, it seems like the LLM stores a state about what was said before. In reality, it is fed the full conversation for every inference, making it seem like the model has a memory of the past.\n",
    "\n",
    "Let's create a *chat_callback* function that takes *new_message* and *message_history* as inputs from the Gradio UI callback and then builds the *messages* input for the OpenAI API. We  will then use it to call the API and generate a response, which we output in the Gradio UI. Just like that, we built an LLM-backed chatbot! We will make it access our database after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7891\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7891/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/blocks.py\", line 1945, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/blocks.py\", line 1768, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 494, in postprocess\n",
      "    return self._postprocess_messages_tuples(cast(TupleFormat, value))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 429, in _postprocess_messages_tuples\n",
      "    self._postprocess_content(message_pair[1]),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 421, in _postprocess_content\n",
      "    raise ValueError(f\"Invalid message for Chatbot component: {chat_message}\")\n",
      "ValueError: Invalid message for Chatbot component: ChatCompletion(id='chatcmpl-AJNQZ3gC8VNHism40wyvrcZljOTeW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I have already listed the products I have information about. Here they are again:\\n\\n1. Thompson Grapes\\n2. Celery\\n3. Peaches\\n4. Potatoes\\n5. Oranges\\n6. Green Leaf Lettuce\\n7. Carrots\\n8. Cantaloupe\\n9. Avocados\\n10. Strawberries\\n11. Romaine Lettuce\\n12. Broccoli Crowns\\n13. Asparagus\\n14. Tomatoes\\n15. Cauliflower\\n16. Plums\\n17. Iceberg Lettuce\\n18. Honeydews\\n19. Red Leaf Lettuce\\n20. Broccoli Bunches\\n21. Flame Grapes\\n22. Nectarines\\n\\nIf you have questions related to the price data of these products, please ask!', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729181651, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_e2bde53e6e', usage=CompletionUsage(completion_tokens=158, prompt_tokens=968, total_tokens=1126, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def chat_callback(new_message: str, message_history: list[tuple[str, str]]):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for user, assistant in message_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": new_message})\n",
    "    return call_openai(messages)\n",
    "\n",
    "def call_openai(messages: list[str]) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat_callback).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a real chat that integrates with OpenAI! Unfortunately, it does not yet know much about the prices in the database. It only knows about the products and sample records from the system prompt. Let's change that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Function Calling to Let LLM Query Database\n",
    "The OpenAI chat API gives us the option to pass a list of tools the model can use. We will have to provide the tool information in a specific format, just like we did with the messages. The model then decides when it wants to use a tool to answer a user query and returns a flag to let us know to call a function. We then call the function ourselves and provide the function output to the LLM in a prompt using the role *tool*. The model will then use that output to answer the user question. You can read the function calling docs here: https://platform.openai.com/docs/guides/function-calling.\n",
    "\n",
    "Let's go step by step.\n",
    "\n",
    "First, we need to describe our functions using a special format the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prices_function = {\n",
    "    \"name\": \"get_current_prices\",\n",
    "    \"description\": \"Gets the most recent prices for a specific product. Call this function when a user asks for the current price of a product.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The product the user wants to know the price for. Make sure to format the value according to the list of distinct products\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"product_name\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "prices_in_date_range_function = {\n",
    "    \"name\": \"get_records_in_date_range\",\n",
    "    \"description\": \"\"\"\n",
    "    Gets the prices for a specific product in a date range. Call this when the user asks questions that require information from a larger \n",
    "    time frame such as 'What was the lowest price for potatoes last year?' or 'On average, where are organges the most expensive historically?'.\n",
    "    \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The product the user searches information for. Make sure to format the value according to the list of distinct products\",\n",
    "            },\n",
    "            \"min_date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The lower boundary of the date range. Make sure to pass ISO 8601 date formats, such as 2019-05-19.\",\n",
    "            },\n",
    "            \"max_date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The upper boundary of the date range. Make sure to pass ISO 8601 date formats, such as 2019-05-19.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"product_name\", \"min_date\", \"max_date\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\"type\": \"function\", \"function\": current_prices_function},\n",
    "    {\"type\": \"function\", \"function\": prices_in_date_range_function},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the API call, we can pass the tools list along, allowing the model to use the tools. If it decides to use a tool to answer a question, it will return a special finish_reason value in the response which we have to check for: \n",
    "\n",
    "`response.choices[0].finish_reason==\"tool_calls\"`\n",
    "\n",
    "If the condition is true, we will have to call a function for the model. It will also provide the correct input arguments, which we will have to extract. Once we called the function with the provided arguments, we pass it the result in the prompt using the role *tools*. The model will use that result to finally answer the user query.\n",
    "\n",
    "Let's rework the *chat_callback* and *call_openai* functions a bit. We will also create a *call_tool* function to handle the tool calls requested by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7893\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def call_openai(messages: list[str], tools: list[str] = None) -> str:\n",
    "    if tools is not None:\n",
    "        return openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "    return openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "\n",
    "def chat_callback(new_message: str, message_history: list[tuple[str, str]]):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for user, assistant in message_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": new_message})\n",
    "    response = call_openai(messages, tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response = call_tool(message.tool_calls[0])\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = call_openai(messages) # Call API again with the function output without providing tools because we would have to implement more complex handling \n",
    "    \n",
    "    return response.choices[0].message.content\n",
    " \n",
    "\n",
    "def call_tool(tool_call: dict):\n",
    "    \"\"\"\n",
    "    This function checks what function to call for the model, extracts the arguments, then calls the function and returns the response\n",
    "    in the format required by the OpenAI API.\n",
    "    \"\"\"\n",
    "    function_name = tool_call.function.name\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    if function_name == \"get_current_prices\":\n",
    "        product_name = arguments.get(\"product_name\")\n",
    "        return { \n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(\n",
    "                {\n",
    "                    \"product_name\": product_name, \n",
    "                    \"current_prices\": get_current_prices(product_name)\n",
    "                }\n",
    "            ),\n",
    "        }\n",
    "    elif function_name == \"get_records_in_date_range\":\n",
    "        product_name = arguments.get(\"product_name\")\n",
    "        min_date = arguments.get(\"min_date\")\n",
    "        max_date = arguments.get(\"max_date\")\n",
    "        return {\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(\n",
    "                {\n",
    "                    \"product_name\": product_name,\n",
    "                    \"min_date\": min_date,\n",
    "                    \"max_date\": max_date,\n",
    "                    \"prices\": get_records_in_date_range(\n",
    "                        product_name = product_name,\n",
    "                        min_date = min_date,\n",
    "                        max_date = max_date\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "    else: \n",
    "        raise Exception(f\"Unknown function {function_name}\")\n",
    "    \n",
    "\n",
    "gr.ChatInterface(fn=chat_callback).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We can now chat with out data. \n",
    "\n",
    "In a real-world scenario, we would probably have to add more functions. We would also have to consider security and data access limitations based on the user role, testing, deployment, rate-limits etc.\n",
    "\n",
    "For a quick POC, this is pretty neat though, don't you think?\n",
    "\n",
    "Also, keep in mind the heavy lifting the model does for us. It even formats query parameters correctly based on our system prompts and function description. This once again shows how powerful these models really are if you wrap them around some system as an abstraction layer. They can make complex tools more easily available to a broader audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
