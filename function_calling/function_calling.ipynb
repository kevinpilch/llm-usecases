{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Function Calling\n",
    "One of the many ways to provide an LLM with real-time, dynamic data is to let it call functions. Or, to be more specifc, let it tell you to call a function for it and then provide the results so it can use it to fulfill a query.\n",
    "\n",
    "In this notebook, I will show you how to implement function calling using the OpenAI API and GPT-4o-mini. We will create an analytics agent that can query data from a database to answer analytical questions we can ask in natural language!\n",
    "\n",
    "I will use DuckDB as a light-weight in-memory database. This would work with any other database, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Some Data\n",
    "I found some product price index data set on Kaggle, which shows the historic price evolution of some foods in different US cities over the course of two decades. Let's load it to a table called product_prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "    \"\"\"\n",
    "    CREATE TABLE product_prices AS\n",
    "    SELECT * FROM read_csv('ProductPriceIndex.csv');\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────┬────────────┬───────────┬───────────────┬───────────────┬──────────────────┬───────────────┬───────────────┐\n",
      "│   productname    │    date    │ farmprice │ atlantaretail │ chicagoretail │ losangelesretail │ newyorkretail │ averagespread │\n",
      "│     varchar      │    date    │  varchar  │    varchar    │    varchar    │     varchar      │    varchar    │    varchar    │\n",
      "├──────────────────┼────────────┼───────────┼───────────────┼───────────────┼──────────────────┼───────────────┼───────────────┤\n",
      "│ Strawberries     │ 2019-05-19 │ $1.16     │ $2.23         │ $1.70         │ $1.99            │ $2.54         │ 82.33%        │\n",
      "│ Romaine Lettuce  │ 2019-05-19 │ $0.35     │ $1.72         │ $2.00         │ $1.69            │ $1.99         │ 428.57%       │\n",
      "│ Red Leaf Lettuce │ 2019-05-19 │ $0.32     │ $1.84         │ $1.84         │ $1.69            │ $1.89         │ 467.19%       │\n",
      "│ Potatoes         │ 2019-05-19 │ $1.50     │ $5.32         │ $5.14         │ $3.99            │ $6.22         │ 244.50%       │\n",
      "│ Oranges          │ 2019-05-19 │ $0.41     │ $1.42         │ $1.45         │ $1.34            │ $2.05         │ 281.71%       │\n",
      "└──────────────────┴────────────┴───────────┴───────────────┴───────────────┴──────────────────┴───────────────┴───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM product_prices LIMIT 5;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬─────────────┐\n",
      "│  min_date  │  max_date  │ num_records │\n",
      "│    date    │    date    │    int64    │\n",
      "├────────────┼────────────┼─────────────┤\n",
      "│ 1999-10-24 │ 2019-05-19 │       15766 │\n",
      "└────────────┴────────────┴─────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT min(date) AS min_date, max(date) AS max_date, COUNT(*) as num_records FROM product_prices;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┐\n",
      "│   dates    │\n",
      "│    date    │\n",
      "├────────────┤\n",
      "│ 2019-05-19 │\n",
      "│ 2019-05-12 │\n",
      "│ 2019-05-05 │\n",
      "│ 2019-04-28 │\n",
      "│ 2019-04-21 │\n",
      "│ 2019-04-14 │\n",
      "│ 2019-04-07 │\n",
      "│ 2019-03-31 │\n",
      "│ 2019-03-24 │\n",
      "│ 2019-03-17 │\n",
      "├────────────┤\n",
      "│  10 rows   │\n",
      "└────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT distinct(date) as dates FROM product_prices ORDER BY dates DESC LIMIT 10;\n",
    "    \"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a column called productname, a date column and a bunch of columns with price information for different cities. The information ranges from 1999 to 2019 and seems to give us an overview of price evolution for different products on a weekly basis. Let's give an LLM the capabilities to query out database!\n",
    "\n",
    "Let's assume this is a production database that changes over time. Products and new price information can be added. The LLM will need to be able to access this information on-demand. We will give it the capabilities to call functions that run queries and return the results for it to use to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Query Functions\n",
    "\n",
    "First, let's create a decorator function *result_to_json* so get the DuckDB query results in JSON format that can be passed to the LLM as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from duckdb.duckdb import DuckDBPyRelation\n",
    "\n",
    "def result_to_json(func: Callable[..., DuckDBPyRelation]) -> Callable[..., str]:\n",
    "    def inner(*args, **kwargs) -> str:\n",
    "        result = func(*args, **kwargs)\n",
    "        return result.fetchdf().to_json(orient=\"records\", date_format=\"iso\")\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I will create functions to do the following things:\n",
    "- Get all distinct products in the database\n",
    "- Current the most recent prices for a specific product\n",
    "- All records for a product in a date range\n",
    "\n",
    "Of course we could pinpoint these capabilities to match business needs and frequent questions. For now, let's assume these are important informations that are requested frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "@result_to_json\n",
    "def get_data_sample() -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        \"\"\"\n",
    "        SELECT * FROM product_prices LIMIT 5;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_distinct_products() -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        \"\"\"\n",
    "        SELECT distinct(productname) FROM product_prices;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_current_prices(product_name: str) -> DuckDBPyRelation:\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT * \n",
    "        FROM product_prices\n",
    "        WHERE productname = \\'{product_name}\\'\n",
    "        AND date = (\n",
    "            SELECT MAX(date)\n",
    "            FROM product_prices\n",
    "            WHERE productname = '{product_name}'\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "@result_to_json\n",
    "def get_records_in_date_range(product_name: str, min_date: str, max_date: str):\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM product_prices\n",
    "        WHERE productname = \\'{product_name}\\'\n",
    "        AND date >= \\'{min_date}\\'\n",
    "        AND date <= \\'{max_date}\\'\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chatbot Using the OpenAI API\n",
    "So far, so good. Now, let's create a simple chatbot that we can interact with that uses the OpenAI API for LLM integration. We will later integrate the above function calls with the LLM, making it a little data analysis agent.\n",
    "\n",
    "First, as always, let's create our system and user prompt to tell the LLM its purpose. In the system prompt, I will use the output of the *get_distinct_products* function so the LLM knows which products it can answer questions for. We will also tell the LLM in the system prompt that today is the max. date of our dataset, 2019-05-19, so we can treat the data as if it is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are a helpful assistant that analyzes data. You will answer questions about the price development of different food products based\n",
    "on data stored in a database. \n",
    "\n",
    "Here is a sample of what the data that you will analyze looks like:\n",
    "{get_data_sample()}\n",
    "\n",
    "The database contains information about the following distinct products: \n",
    "{get_distinct_products()}\n",
    "\n",
    "Today's date is 2019-05-19. Data is available from 1999-10-24 til 2019-05-19. \n",
    "\n",
    "Only answer questions related to analyzing the product price data. If the user requests you to do anything else, \n",
    "kindly tell them that you are here to analyze product price data and that you cannot help with other requests.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant that analyzes data. You will answer questions about the price development of different food products based\n",
      "on data stored in a database. \n",
      "\n",
      "Here is a sample of what the data that you will analyze looks like:\n",
      "[{\"productname\":\"Strawberries\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$1.16\",\"atlantaretail\":\"$2.23\",\"chicagoretail\":\"$1.70\",\"losangelesretail\":\"$1.99\",\"newyorkretail\":\"$2.54\",\"averagespread\":\"82.33%\"},{\"productname\":\"Romaine Lettuce\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.35\",\"atlantaretail\":\"$1.72\",\"chicagoretail\":\"$2.00\",\"losangelesretail\":\"$1.69\",\"newyorkretail\":\"$1.99\",\"averagespread\":\"428.57%\"},{\"productname\":\"Red Leaf Lettuce\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.32\",\"atlantaretail\":\"$1.84\",\"chicagoretail\":\"$1.84\",\"losangelesretail\":\"$1.69\",\"newyorkretail\":\"$1.89\",\"averagespread\":\"467.19%\"},{\"productname\":\"Potatoes\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$1.50\",\"atlantaretail\":\"$5.32\",\"chicagoretail\":\"$5.14\",\"losangelesretail\":\"$3.99\",\"newyorkretail\":\"$6.22\",\"averagespread\":\"244.50%\"},{\"productname\":\"Oranges\",\"date\":\"2019-05-19T00:00:00.000\",\"farmprice\":\"$0.41\",\"atlantaretail\":\"$1.42\",\"chicagoretail\":\"$1.45\",\"losangelesretail\":\"$1.34\",\"newyorkretail\":\"$2.05\",\"averagespread\":\"281.71%\"}]\n",
      "\n",
      "The database contains information about the following distinct products: \n",
      "[{\"productname\":\"Thompson Grapes\"},{\"productname\":\"Celery\"},{\"productname\":\"Peaches\"},{\"productname\":\"Potatoes\"},{\"productname\":\"Oranges\"},{\"productname\":\"Green Leaf Lettuce\"},{\"productname\":\"Carrots\"},{\"productname\":\"Cantaloupe\"},{\"productname\":\"Avocados\"},{\"productname\":\"Strawberries\"},{\"productname\":\"Romaine Lettuce\"},{\"productname\":\"Broccoli Crowns\"},{\"productname\":\"Asparagus\"},{\"productname\":\"Tomatoes\"},{\"productname\":\"Cauliflower\"},{\"productname\":\"Plums\"},{\"productname\":\"Iceberg Lettuce\"},{\"productname\":\"Honeydews\"},{\"productname\":\"Red Leaf Lettuce\"},{\"productname\":\"Broccoli Bunches\"},{\"productname\":\"Flame Grapes\"},{\"productname\":\"Nectarines\"}]\n",
      "\n",
      "Today's date is 2019-05-19. Data is available from 1999-10-24 til 2019-05-19. \n",
      "\n",
      "Only answer questions related to analyzing the product price data. If the user requests you to do anything else, \n",
      "kindly tell them that you are here to analyze product price data and that you cannot help with other requests.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the user prompt dynamic, we will leverage a Gradio chat UI that dynamically passes the user input, the previous conversation, as well\n",
    "as the system prompt to the OpenAI model. \n",
    "\n",
    "The Gradio ChatInterface object expects us to pass a callback method that takes exactly two arguments: message and history. The first one is the new user input, while the history is the whole previous conversation. For more information, you can check the Gradio docs here: https://www.gradio.app/docs/gradio/chatinterface\n",
    "\n",
    "For now, let's create the callback method for the chat interface and use it to launch the chat interface. In this method, we will use the user message, the message history and the system prompt to build our messages input for GPT-4o-mini. Like always, we will begin crafting our model input passing a list of JSON objects that looks as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": <SYSTEM_PROMPT>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE>}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now allow the model to see the full previous conversation, we will extend that list by additional objects. For each new response by the model, we will add an object that uses the role *assistant*. This feature can be used to pass the full previous conversation, but also to pass responses from other agents that assist fulfilling a user query. For every new user message, we will append an object that uses the role *user*. \n",
    "\n",
    "Over time, the messages input for the OpenAI API will look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": <SYSTEM_PROMPT>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE_1>},\n",
    "    {\"role\": \"assistant\", \"content\": <GPT_RESPONSE_1>},\n",
    "    {\"role\": \"user\", \"content\": <USER_MESSAGE_2>},\n",
    "    {\"role\": \"assistant\", \"content\": <GPT_RESPONSE_2>},\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing the full conversation every time, the chatbot pretends to hold a full conversation that somehow stores a state. In reality, it is fed the full conversation for every inference and responds based on what was said before.\n",
    "\n",
    "Let's create a *chat_callback* function that takes *new_message* and *message_history* as an input from the Gradio UI and then builds the messages input for the OpenAI API. The function will then use it to call the API and generate a response, which it returns to Gradio. Just like that, we build an LLM-backed chatbot! We will make it access our database after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def chat_callback(new_message, message_history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for user, assistant in message_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": new_message})\n",
    "    return call_openai(messages)\n",
    "\n",
    "def call_openai(messages: list[str]) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat_callback).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You will be able to ask some questions about our data based on the info the model has from the system prompt. This is pretty limiting though. Let's allow it to query data by enabling it to call functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Function Calling to Let LLM Query Database\n",
    "As the model cannot yet query the table, it is not really useful to analyze our data. We will need to provide it with capabilities to use our query functions. Here's how:\n",
    "\n",
    "The OpenAI chat API gives us the option to pass a list of tools the model can use. We will have to provide the tool information in a specific way, just like we did with the messages. The model then decides if it wants to use a tool to answer a user query and returns a flag to let us know to call a function. We then call the function for the LLM, provide the function output to the LLM as a prompt using a specific format again, and it will then use that output to answer the user question. You can read the docs here: https://platform.openai.com/docs/guides/function-calling.\n",
    "\n",
    "Let's go step by step. \n",
    "\n",
    "First, we need to describe our functions using a special format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prices_function = {\n",
    "    \"name\": \"get_current_prices\",\n",
    "    \"description\": \"Gets the most recent prices for a specific product. Call this function when a user asks for the current price of a product.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The product the user wants to know the price for. Make sure to format the value according to the list of distinct products\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"product_name\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "prices_in_date_range_function = {\n",
    "    \"name\": \"get_records_in_date_range\",\n",
    "    \"description\": \"\"\"\n",
    "    Gets the prices for a specific product in a date range. Call this when the user asks questions that require information from a larger \n",
    "    time frame such as 'What was the lowest price for potatoes last year?' or 'On average, where are organges the most expensive historically?'.\n",
    "    \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The product the user searches information for. Make sure to format the value according to the list of distinct products\",\n",
    "            },\n",
    "            \"min_date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The lower boundary of the date range. Make sure to pass ISO 8601 date formats, such as 2019-05-19.\",\n",
    "            },\n",
    "            \"max_date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The upper boundary of the date range. Make sure to pass ISO 8601 date formats, such as 2019-05-19.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"product_name\", \"min_date\", \"max_date\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\"type\": \"function\", \"function\": current_prices_function},\n",
    "    {\"type\": \"function\", \"function\": prices_in_date_range_function},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the API call, we can pass the tools list along, allowing the model to use the tools. If it decides to use a tool to answer a question, it will return a special finish_reason value in the response which we have to check for: \n",
    "\n",
    "`response.choices[0].finish_reason==\"tool_calls\"`\n",
    "\n",
    "In that case, we will have to call a function for it. It will also provide the correct input arguments in the response, which we will have to extract. Once we called the function with the provided arguments for the model, we pass it the response using the role *tools*. It will use that response to finally answer the user query.\n",
    "\n",
    "Let's rework the *chat_callback* and *call_openai* functions a bit. We will also create a *call_tool* function to handle the tool calls requested by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7888\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpilch/Library/Caches/pypoetry/virtualenvs/llm-usecases-n3dvwjYK-py3.12/lib/python3.12/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def call_openai(messages: list[str], tools: list[str] = None) -> str:\n",
    "    if tools is not None:\n",
    "        return openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "    return openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "\n",
    "def chat_callback(new_message: str, message_history: list[tuple[str, str]]):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for user, assistant in message_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": new_message})\n",
    "    response = call_openai(messages, tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response = call_tool(message.tool_calls[0])\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = call_openai(messages) # Call API again with the function output without providing tools because we would have to implement more complex handling \n",
    "    \n",
    "    return response.choices[0].message.content\n",
    " \n",
    "\n",
    "def call_tool(tool_call: dict):\n",
    "    \"\"\"\n",
    "    This function checks what function to call for the model, extracts the arguments, then calls the function and returns the response\n",
    "    in the format required by the OpenAI API.\n",
    "    \"\"\"\n",
    "    function_name = tool_call.function.name\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    if function_name == \"get_current_prices\":\n",
    "        product_name = arguments.get(\"product_name\")\n",
    "        return { \n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(\n",
    "                {\n",
    "                    \"product_name\": product_name, \n",
    "                    \"current_prices\": get_current_prices(product_name)\n",
    "                }\n",
    "            ),\n",
    "        }\n",
    "    elif function_name == \"get_records_in_date_range\":\n",
    "        product_name = arguments.get(\"product_name\")\n",
    "        min_date = arguments.get(\"min_date\")\n",
    "        max_date = arguments.get(\"max_date\")\n",
    "        return {\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(\n",
    "                {\n",
    "                    \"product_name\": product_name,\n",
    "                    \"min_date\": min_date,\n",
    "                    \"max_date\": max_date,\n",
    "                    \"prices\": get_records_in_date_range(\n",
    "                        product_name = product_name,\n",
    "                        min_date = min_date,\n",
    "                        max_date = max_date\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "    else: \n",
    "        raise Exception(f\"Unknown function {function_name}\")\n",
    "    \n",
    "\n",
    "gr.ChatInterface(fn=chat_callback).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!!! We can now chat with out data. In a real-world scenario, we would probably have to add more functions of course. We would also have to consider security and data access limitations based on the user etc. etc.\n",
    "\n",
    "For a quick POC, this is pretty neat though, don't you think? \n",
    "\n",
    "Also, think about the heavy lifting the model does for us. It even formats query parameters correctly based on our system prompts and function description. This shows again how powerful these models really are if you wrap them around some system that usually requires quite some technical knowledge to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
